# Monohuman - Animatable Human
Creating realistic and coherent animations for virtual avatars with free-view control is essential in applications such as virtual reality and digital entertainment. Previous research has explored the use of neural radiance fields (NeRF) to reconstruct human bodies from monocular videos. Some recent approaches integrate deformation networks into NeRF to capture the dynamics of the human neural field for lifelike motions. However, existing methods either rely on pose-dependent representations or struggle with motion coherency, hindering their ability to realistically generalize to new pose sequences. In this paper, we introduce a novel framework called **MonoHuman** that efficiently generates avatars with view-consistency and high fidelity across arbitrary poses. Our approach involves modeling the deformation field with bi-directional constraints and leveraging keyframe information to enhance feature correlations for cohesive results. We propose a Shared Bidirectional Deformation module, which establishes a pose-independent and generalizable deformation field by separating backward and forward deformation correspondences into shared skeletal motion weight and distinct non-rigid motions. Additionally, we introduce a Forward Correspondence Search module that utilizes keyframe information to guide the rendering network. As a result, our rendered avatars exhibit multi-view consistency and high fidelity, even in challenging novel pose scenarios. Extensive experiments demonstrate the superior performance of MonoHuman compared to state-of-the-art methods.

Link to original project Github: https://github.com/Yzmblog/MonoHuman

## Easy Steps To Execute
